{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9607d72-150f-41ab-b662-fbe875761847",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "C:\\Users\\Dell\\anaconda 3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [00:03:47] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\Dell\\anaconda 3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 670ms/step - accuracy: 0.4387 - loss: 1.3453 - val_accuracy: 0.9600 - val_loss: 0.8509\n",
      "Epoch 2/5\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 499ms/step - accuracy: 0.9132 - loss: 0.6014 - val_accuracy: 1.0000 - val_loss: 0.0754\n",
      "Epoch 3/5\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 449ms/step - accuracy: 0.9954 - loss: 0.0500 - val_accuracy: 1.0000 - val_loss: 0.0078\n",
      "Epoch 4/5\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 524ms/step - accuracy: 1.0000 - loss: 0.0059 - val_accuracy: 1.0000 - val_loss: 0.0030\n",
      "Epoch 5/5\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 537ms/step - accuracy: 1.0000 - loss: 0.0032 - val_accuracy: 1.0000 - val_loss: 0.0016\n",
      "Epoch 1/5\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 1s/step - accuracy: 0.4892 - loss: 1.2903 - val_accuracy: 0.9700 - val_loss: 0.3850\n",
      "Epoch 2/5\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 1s/step - accuracy: 0.9736 - loss: 0.2193 - val_accuracy: 0.8150 - val_loss: 0.4526\n",
      "Epoch 3/5\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 1s/step - accuracy: 0.9380 - loss: 0.2157 - val_accuracy: 1.0000 - val_loss: 0.0374\n",
      "Epoch 4/5\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 992ms/step - accuracy: 1.0000 - loss: 0.0240 - val_accuracy: 1.0000 - val_loss: 0.0044\n",
      "Epoch 5/5\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0033 - val_accuracy: 1.0000 - val_loss: 0.0018\n",
      "\u001b[1m7/7\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 259ms/step\n",
      "\u001b[1m7/7\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 422ms/step\n",
      "WARNING:tensorflow:5 out of the last 15 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000028CFED38220> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m7/7\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 185ms/step\n",
      "âœ… Random Forest Accuracy: 1.00\n",
      "âœ… XGBoost Accuracy: 1.00\n",
      "âœ… SVM Accuracy: 1.00\n",
      "âœ… Deep Learning Model Accuracy: 1.00\n",
      "âœ… BiLSTM Accuracy: 1.00\n",
      "âœ… Hybrid Model Accuracy: 0.26\n"
     ]
    }
   ],
   "source": [
    "# ğŸ“Œ Step 1: Import Required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import tensorflow as tf\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, Embedding, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# ğŸ“Œ Step 2: Load Dataset\n",
    "df = pd.read_csv(\"AI_Resume_Screening.csv\")\n",
    "\n",
    "# ğŸ“Œ Step 3: Preprocess Data\n",
    "# ğŸ”¹ Convert text to lowercase\n",
    "df[\"clean_resume\"] = df[\"Skills\"].apply(lambda x: x.lower())\n",
    "\n",
    "# ğŸ”¹ Remove special characters, numbers, and punctuation\n",
    "df[\"clean_resume\"] = df[\"clean_resume\"].apply(lambda x: re.sub(r'[^a-zA-Z\\s]', '', x))\n",
    "\n",
    "# ğŸ”¹ Tokenization and stopword removal\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "df[\"clean_resume\"] = df[\"clean_resume\"].apply(lambda x: \" \".join([word for word in x.split() if word not in stop_words]))\n",
    "\n",
    "# ğŸ”¹ Lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download(\"wordnet\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "df[\"clean_resume\"] = df[\"clean_resume\"].apply(lambda x: \" \".join([lemmatizer.lemmatize(word) for word in x.split()]))\n",
    "\n",
    "# ğŸ”¹ Encode Job Titles\n",
    "label_encoder = LabelEncoder()\n",
    "df[\"job_title_encoded\"] = label_encoder.fit_transform(df[\"Job Role\"])\n",
    "\n",
    "# ğŸ”¹ Save Label Encoder\n",
    "joblib.dump(label_encoder, \"label_encoders.pkl\")\n",
    "\n",
    "# ğŸ“Œ Step 4: Prepare Training Data\n",
    "X_text = df[\"clean_resume\"]  # Input resumes (text data)\n",
    "y = df[\"job_title_encoded\"]  # Target variable (Job Role)\n",
    "\n",
    "# ğŸ”¹ Convert text into numerical features using **TF-IDF** for ML models\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_tfidf = vectorizer.fit_transform(X_text)\n",
    "\n",
    "# ğŸ”¹ Save TF-IDF Vectorizer\n",
    "joblib.dump(vectorizer, \"tfidf_vectorizer.pkl\")\n",
    "\n",
    "# ğŸ”¹ Split into training and testing sets (For ML models)\n",
    "X_train_ml, X_test_ml, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# ğŸ“Œ Step 5: Train & Save Machine Learning Models\n",
    "## âœ… Random Forest Model (TF-IDF Features)\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train_ml, y_train)\n",
    "joblib.dump(rf_model, \"random_forest_model.pkl\")\n",
    "\n",
    "## âœ… XGBoost Model (TF-IDF Features)\n",
    "xgb_model = XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=5, use_label_encoder=False, eval_metric=\"mlogloss\")\n",
    "xgb_model.fit(X_train_ml, y_train)\n",
    "joblib.dump(xgb_model, \"xgboost_model.pkl\")\n",
    "\n",
    "## âœ… SVM Model (TF-IDF Features)\n",
    "svm_model = SVC(kernel=\"linear\", probability=True)\n",
    "svm_model.fit(X_train_ml, y_train)\n",
    "joblib.dump(svm_model, \"svm_model.pkl\")\n",
    "\n",
    "# ğŸ“Œ Step 6: Prepare Deep Learning Input Features\n",
    "# ğŸ”¹ Tokenize and Pad Sequences\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(df[\"clean_resume\"])\n",
    "X_seq = tokenizer.texts_to_sequences(df[\"clean_resume\"])\n",
    "X_padded = pad_sequences(X_seq, maxlen=300)\n",
    "\n",
    "# ğŸ”¹ Split into training and testing sets (For Deep Learning models)\n",
    "X_train_dl, X_test_dl, y_train, y_test = train_test_split(X_padded, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# âœ… Save Tokenizer\n",
    "joblib.dump(tokenizer, \"tokenizer.pkl\")\n",
    "\n",
    "# ğŸ“Œ Step 7: Train & Save Deep Learning Models\n",
    "# âœ… Deep Learning Model (DNN)\n",
    "deep_model = Sequential([\n",
    "    Embedding(5000, 128, input_length=300),\n",
    "    LSTM(128, return_sequences=True),\n",
    "    Dropout(0.3),\n",
    "    LSTM(64),\n",
    "    Dense(64, activation=\"relu\"),\n",
    "    Dense(len(label_encoder.classes_), activation=\"softmax\")\n",
    "])\n",
    "deep_model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=Adam(learning_rate=0.001), metrics=[\"accuracy\"])\n",
    "deep_model.fit(X_train_dl, y_train, epochs=5, batch_size=32, validation_data=(X_test_dl, y_test))\n",
    "deep_model.save(\"deep_learning_model.keras\")\n",
    "\n",
    "# âœ… BiLSTM Model\n",
    "bilstm_model = Sequential([\n",
    "    Embedding(5000, 128, input_length=300),\n",
    "    Bidirectional(LSTM(128, return_sequences=True)),\n",
    "    Dropout(0.3),\n",
    "    Bidirectional(LSTM(64)),\n",
    "    Dense(64, activation=\"relu\"),\n",
    "    Dense(len(label_encoder.classes_), activation=\"softmax\")\n",
    "])\n",
    "bilstm_model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=Adam(learning_rate=0.001), metrics=[\"accuracy\"])\n",
    "bilstm_model.fit(X_train_dl, y_train, epochs=5, batch_size=32, validation_data=(X_test_dl, y_test))\n",
    "bilstm_model.save(\"bilstm_model.keras\")\n",
    "\n",
    "# ğŸ“Œ Step 8: Hybrid Model (XGBoost + Deep Learning)\n",
    "def hybrid_predict(X):\n",
    "    xgb_preds = xgb_model.predict(X)\n",
    "    deep_preds = np.argmax(deep_model.predict(X), axis=1)\n",
    "    return np.round((xgb_preds + deep_preds) / 2).astype(int)\n",
    "\n",
    "# Save Hybrid Model\n",
    "joblib.dump(hybrid_predict, \"hybrid_model.pkl\")\n",
    "\n",
    "# ğŸ“Œ Step 9: Save Scaler & Encoders\n",
    "scaler = StandardScaler()\n",
    "joblib.dump(scaler, \"scaler.pkl\")\n",
    "\n",
    "# ğŸ“Œ Step 10: Evaluate Models\n",
    "rf_accuracy = accuracy_score(y_test, rf_model.predict(X_test_ml))\n",
    "xgb_accuracy = accuracy_score(y_test, xgb_model.predict(X_test_ml))\n",
    "svm_accuracy = accuracy_score(y_test, svm_model.predict(X_test_ml))\n",
    "deep_accuracy = accuracy_score(y_test, np.argmax(deep_model.predict(X_test_dl), axis=1))\n",
    "bilstm_accuracy = accuracy_score(y_test, np.argmax(bilstm_model.predict(X_test_dl), axis=1))\n",
    "hybrid_accuracy = accuracy_score(y_test, hybrid_predict(X_test_ml))\n",
    "\n",
    "print(f\"âœ… Random Forest Accuracy: {rf_accuracy:.2f}\")\n",
    "print(f\"âœ… XGBoost Accuracy: {xgb_accuracy:.2f}\")\n",
    "print(f\"âœ… SVM Accuracy: {svm_accuracy:.2f}\")\n",
    "print(f\"âœ… Deep Learning Model Accuracy: {deep_accuracy:.2f}\")\n",
    "print(f\"âœ… BiLSTM Accuracy: {bilstm_accuracy:.2f}\")\n",
    "print(f\"âœ… Hybrid Model Accuracy: {hybrid_accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bf640f-0ca6-4f09-8295-da0a2be48394",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
